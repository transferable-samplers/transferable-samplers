# @package _global_

defaults:
  - override /model: normalizing_flow
  - override /data: transferable/transferable/up_to_8aa
  - override /trainer: gpu

train: False
val: False
test: True
task_name: eval

model:
  sampling_config:
    use_com_adjustment: True
    batch_size: 2_500
    num_test_proposal_samples: 10_000
    num_smc_samples: 10_000
    clip_reweighting_logits: 0.002 # it was working well on AC without - lets try avoid
  net:
    _target_: src.models.neural_networks.tarflow.tarflow.TarFlow
    input_dimension: 3 # 3D coordinates
    channels: 384
    max_num_tokens: ${data.num_atoms}
    num_blocks: 8
    layers_per_block: 8
    permutation_keys:
    - "n2c_backbone-first_standard"
    - "n2c_residue-by-residue_standard_flip"
    - "n2c_backbone-first_standard_flip"
    - "n2c_residue-by-residue_standard"
    - "n2c_backbone-first_variant"
    - "n2c_residue-by-residue_variant_flip"
    - "n2c_backbone-first_variant_flip"
    - "n2c_residue-by-residue_variant"
    cond_embed:
      _target_: src.models.neural_networks.embedder.ConditionalEmbedder
      hidden_dim: ${model.net.channels}
      output_dim: ${model.net.channels}
      sinusoid_div_value: 1000 # for sinusoidal positional embedding
    use_adapt_ln: True
    use_transition: True
    use_qkln: True
    pos_embed_type: "sinusoidal"
    lookahead_conditioning: True
    
  smc_sampler:
    _target_: src.models.samplers.ula_sampler.SMCSamplerULA
    enabled: True
    langevin_eps: 1e-7
    num_timesteps: 100
    ess_threshold: 1
    systematic_resampling: True
    warmup: 0.
    batch_size: 32 # set for 48gb
    input_energy_cutoff: 100 #Â this might need tuning if you get NaNs!
    do_energy_plots: True
    log_freq: 1

data:
  batch_size: 128
  num_eval_samples: 10_000

trainer:
  limit_test_batches: 1
  limit_val_batches: 1
