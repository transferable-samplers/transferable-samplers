import pytorch_lightning as pl
from typing import Any, Dict

import logging
from typing import Callable, Optional

from src.data.preprocessing.tica import TicaModel
from src.evaluation.metrics.evaluate_peptide_data import evaluate_peptide_data
from src.evaluation.plots.plot_atom_distances import plot_atom_distances
from src.evaluation.plots.plot_com_norms import plot_com_norms
from src.evaluation.plots.plot_energies import plot_energies
from src.evaluation.plots.plot_ramachandran import plot_ramachandran
from src.evaluation.plots.plot_tica import plot_tica
from src.utils.data_types import SamplesData


class EnsembleEvaluator:
    def __init__(self, fix_symmetry: bool = True, drop_unfixable_symmetry: bool = False, num_eval_samples: int = 10_000):
        self.fix_symmetry = fix_symmetry
        self.drop_unfixable_symmetry = drop_unfixable_symmetry

    def compute_metrics(self, true_data, pred_data, topology, tica_model, prefix: str = ""):
        """Computes all metrics between true and predicted data."""

        metrics = {}

        if len(pred_data) < 0.9 * num_eval_samples:
            logging.warning(r"Less than 90% of required eval samples supplied.")

        # Slice data to subset
        if num_eval_samples is None:
            num_eval_samples = min(len(pred_data), len(true_data))
        else:
            num_eval_samples = min(num_eval_samples, len(pred_data), len(true_data))
        true_data = true_data[:num_eval_samples]
        pred_data = pred_data[:num_eval_samples]
        metrics[f"{prefix}/num_eval_samples"] = min(num_eval_samples, len(pred_data))

        # Compute effective sample size
        if pred_data.logits is not None:
            ess = sampling_efficiency(pred_data.logits)
            metrics[f"{prefix}/effective_sample_size"] = ess

        # Energy metrics
        metrics.update(
            energy_distances(
                true_data.energy,
                pred_data.energy,
                prefix=prefix,
            )
        )
        metrics[f"{prefix}/mean_energy"] = pred_data.energy.mean().cpu()
        metrics[f"{prefix}/median_energy"] = pred_data.energy.median().cpu()
        logging.info("Energy metrics computed")

        # Ramachandran metrics
        metrics.update(ramachandran_metrics(true_data.samples, pred_data.samples, topology, prefix=prefix))
        logging.info("Ramachandran metrics computed")

        # TICA metric
        metrics.update(tica_metric(true_data.samples, pred_data.samples, topology, tica_model, prefix=prefix))
        logging.info("TICA metrics computed")

        # JSD metric
        metrics.update(jsd_metric(true_data.samples, pred_data.samples, topology, tica_model=tica_model, prefix=prefix))
        logging.info("JSD metrics computed")

        return metrics


    def evaluate(self, sequence, samples_data_dict, evaluation_inputs, energy_fn):
        """
    Compute evaluation metrics and log diagnostic plots for a single peptide sequence.

    Logs Ramachandran plots, TICA projections, energy distributions, atom
    distance distributions, and center-of-mass norms for provided datasets.
    Also computes quantitative evaluation metrics by comparing generated
    samples against true reference data.

    Args:
        log_image_fn (Callable): Function used to log or save generated plots.
        sequence (str): Peptide sequence identifier.
        topology: Topology object for the peptide.
        tica_model (TicaModel): TICA model used for projections.
        num_eval_samples (int): Number of samples to use for evaluation.
        true_data (SamplesData): Reference trajectory samples and energies.
        proposal_data (SamplesData): Samples generated by the proposal distribution.
        resampled_data (SamplesData): Samples after resampling.
        smc_data (Optional[SamplesData], optional): Samples from SMC if available.
        do_plots (bool, optional): Whether to generate plots. Defaults to True.
        prefix (str, optional): String prefix to prepend to log keys.

    Returns:
        dict: Dictionary mapping metric names to computed values.
    """

        topology = evaluation_inputs.topology
        tica_model = evaluation_inputs.tica_model
        true_samples = evaluation_inputs.true_samples

        metrics = {}
        plots = {}

        if self.fix_symmetry:
            for name, samples_data in samples_data_dict.items():
                samples_data[name] = fix_chirality(true_samples, samples_data[name], topology)

        for name, samples_data in samples_data_dict.items():
            logging.info(f"Evaluating {sequence}/{name} samples")
            if len(samples_data) == 0:
                logging.warning(f"No {samples_type} samples present.")
                continue

            metrics[samples_type] = self.compute_metrics(
                true_samples,
                samples_data,
                topology,
                tica_model,
                num_eval_samples=self.num_eval_samples,
                prefix=f"{sequence}/{samples_type}",
            )
            plots[samples_type] = {
                "ramachandran": plot_ramachandran(samples_data.samples, topology, prefix=prefix + name),
                "tica": plot_tica(
                    samples_data.samples,
                    topology,
                    tica_model=tica_model,
                    prefix=prefix + name,
                ),
            }

        plots["energies"] = plot_energies(
            samples_data_dict,
            num_samples=self.num_eval_samples,
            prefix=prefix,
        )
        plots["atom_distances"] = plot_atom_distances(
            samples_data_dict,
            num_samples=self.num_eval_samples,
            prefix=prefix,
        )
        plots["com_norms"] = plot_com_norms(
            samples_data_dict,
            num_samples=self.num_eval_samples,
            prefix=prefix,
        )

        return metrics, plots


